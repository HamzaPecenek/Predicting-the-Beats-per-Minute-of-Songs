{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1) SETUP\nimport os, math, gc, warnings, json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import Ridge\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(context=\"notebook\", style=\"whitegrid\")\npd.set_option(\"display.float_format\", lambda x: f\"{x:,.6f}\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR = \"/kaggle/input/playground-series-s5e9/\"\nTRAIN = os.path.join(DATA_DIR, \"train.csv\")\nTEST  = os.path.join(DATA_DIR, \"test.csv\")\nSUB_SAMP = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\nTARGET = \"BeatsPerMinute\"\nID_COL = \"id\"\n\n# Optional libs\nHAS_LGB = True\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    HAS_LGB = False\n    print(\"LightGBM not available; skipping that model.\")\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\nrmse_scorer = make_scorer(lambda yt, yp: rmse(yt, yp), greater_is_better=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:39.528903Z","iopub.execute_input":"2025-09-06T19:19:39.529265Z","iopub.status.idle":"2025-09-06T19:19:45.986663Z","shell.execute_reply.started":"2025-09-06T19:19:39.529243Z","shell.execute_reply":"2025-09-06T19:19:45.985777Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 2) LOAD DATA & DEFINE FEATURES\ntrain = pd.read_csv(TRAIN)\ntest  = pd.read_csv(TEST)\n\nprint(\"Shapes:\", train.shape, test.shape)\nassert TARGET in train.columns and ID_COL in train.columns and ID_COL in test.columns\n\n# Numeric features only, excluding id/target\nnumeric_cols = [c for c in train.columns \n                if c not in [TARGET, ID_COL] and pd.api.types.is_numeric_dtype(train[c])]\nprint(\"Using numeric features:\", numeric_cols)\n\nX = train[numeric_cols].astype(\"float32\")\ny = train[TARGET].astype(\"float32\").values\nX_test = test[numeric_cols].astype(\"float32\")\n\nbpm_min, bpm_max = float(train[TARGET].min()), float(train[TARGET].max())\nprint(f\"BPM range (train): [{bpm_min:.3f}, {bpm_max:.3f}]\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:45.988009Z","iopub.execute_input":"2025-09-06T19:19:45.988647Z","iopub.status.idle":"2025-09-06T19:19:47.901449Z","shell.execute_reply.started":"2025-09-06T19:19:45.988624Z","shell.execute_reply":"2025-09-06T19:19:47.900642Z"}},"outputs":[{"name":"stdout","text":"Shapes: (524164, 11) (174722, 10)\nUsing numeric features: ['RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality', 'InstrumentalScore', 'LivePerformanceLikelihood', 'MoodScore', 'TrackDurationMs', 'Energy']\nBPM range (train): [46.718, 206.037]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3) CV SETUP + EVALUATOR\nCV_SPLITS = 5\ncv = KFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\ndef evaluate(name, estimator, X, y, cv=cv, scorer=rmse_scorer):\n    scores = -cross_val_score(estimator, X, y, cv=cv, scoring=scorer, n_jobs=-1)\n    print(f\"{name:18s} RMSE (CV={cv.get_n_splits()}): {scores.mean():.4f} ± {scores.std():.4f}\")\n    return {\"model\": name, \"rmse_mean\": scores.mean(), \"rmse_std\": scores.std(), \"estimator\": estimator}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:47.902393Z","iopub.execute_input":"2025-09-06T19:19:47.902645Z","iopub.status.idle":"2025-09-06T19:19:47.908039Z","shell.execute_reply.started":"2025-09-06T19:19:47.902626Z","shell.execute_reply":"2025-09-06T19:19:47.907156Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 3B) FAST CV SAMPLE (bounds runtime)\nFAST_CV = True\nCV_SAMPLE = 300_000   # adjust smaller if needed (e.g., 100_000)\n\nif FAST_CV and len(X) > CV_SAMPLE:\n    cv_idx = np.random.RandomState(42).choice(len(X), CV_SAMPLE, replace=False)\n    X_cv = X.iloc[cv_idx].copy()\n    y_cv = y[cv_idx].copy()\nelse:\n    X_cv = X\n    y_cv = y\n\nprint(f\"CV will use {len(X_cv):,} rows out of {len(X):,}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:47.910525Z","iopub.execute_input":"2025-09-06T19:19:47.910806Z","iopub.status.idle":"2025-09-06T19:19:48.008298Z","shell.execute_reply.started":"2025-09-06T19:19:47.910788Z","shell.execute_reply":"2025-09-06T19:19:48.007545Z"}},"outputs":[{"name":"stdout","text":"CV will use 300,000 rows out of 524,164.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 4) DEFINE FAST MODELS\nmodels = []\n\n# A) Dummy (mean)\nmodels.append((\"Dummy(mean)\", DummyRegressor(strategy=\"median\")))\n\n# B) Ridge on raw features (standardized)\nmodels.append((\n    \"Ridge\",\n    Pipeline([\n        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n        (\"ridge\", Ridge(alpha=1.0, random_state=RANDOM_STATE))\n    ])\n))\n\n# C) Ridge on polynomial degree=2 (captures simple interactions; still fast here)\n#    Keep degree=2 to limit feature explosion (≈ features + squares + pairwise inter.).\nmodels.append((\n    \"Poly2+Ridge\",\n    Pipeline([\n        (\"poly\",   PolynomialFeatures(degree=2, include_bias=False)),\n        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n        (\"ridge\",  Ridge(alpha=1.0, random_state=RANDOM_STATE))\n    ])\n))\n\n# D) LightGBM (tree ensemble) — small, fast config\nif HAS_LGB:\n    lgbm = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1000,          # modest rounds\n        learning_rate=0.025,\n        num_leaves=31,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    )\n    models.append((\"LightGBM\", lgbm))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:48.009026Z","iopub.execute_input":"2025-09-06T19:19:48.009257Z","iopub.status.idle":"2025-09-06T19:19:48.016614Z","shell.execute_reply.started":"2025-09-06T19:19:48.009240Z","shell.execute_reply":"2025-09-06T19:19:48.015691Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 5) FAST CROSS-VALIDATION (RMSE) with bounded data + LGBM early stopping\ncv_results = []\n\ndef evaluate_fast(name, estimator, Xc, yc, cv=cv):\n    # Generic sklearn models via cross_val_score (on sampled rows)\n    scores = -cross_val_score(estimator, Xc, yc, cv=cv, scoring=rmse_scorer, n_jobs=-1)\n    print(f\"{name:18s} RMSE (CV={cv.get_n_splits()}): {scores.mean():.4f} ± {scores.std():.4f}\")\n    return {\"model\": name, \"rmse_mean\": scores.mean(), \"rmse_std\": scores.std(), \"estimator\": estimator}\n\n# A) Dummy\nfor name, est in models:\n    if name != \"LightGBM\":\n        res = evaluate_fast(name, est, X_cv, y_cv, cv=cv)\n        res[\"name\"] = name\n        cv_results.append(res)\n\n# B) LightGBM with early stopping (custom loop)\nif HAS_LGB and any(n == \"LightGBM\" for n, _ in models):\n    params = dict(\n        objective=\"regression\",\n        learning_rate=0.025,\n        num_leaves=31,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    )\n    NUM_BOOST_ROUND = 1000\n    EARLY_STOP = 300\n\n    rmses, iters = [], []\n    for fold, (tr_idx, va_idx) in enumerate(cv.split(X_cv), 1):\n        X_tr, X_va = X_cv.iloc[tr_idx].values, X_cv.iloc[va_idx].values\n        y_tr, y_va = y_cv[tr_idx], y_cv[va_idx]\n\n        model = lgb.LGBMRegressor(**params, n_estimators=NUM_BOOST_ROUND)\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(EARLY_STOP, verbose=False)],\n        )\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n        fold_rmse = rmse(y_va, pred_va)\n        rmses.append(fold_rmse)\n        iters.append(model.best_iteration_)\n\n        print(f\"LightGBM (fold {fold})  RMSE={fold_rmse:.4f}  best_iter={model.best_iteration_}\")\n\n    rmse_mean, rmse_std = float(np.mean(rmses)), float(np.std(rmses))\n    best_iter_median = int(np.median(iters))\n    print(f\"LightGBM (early-stop)    RMSE (CV={cv.get_n_splits()}): {rmse_mean:.4f} ± {rmse_std:.4f} | median_iter={best_iter_median}\")\n\n    # store an unfitted template with tuned n_estimators for final training\n    tuned_lgbm = lgb.LGBMRegressor(**params, n_estimators=best_iter_median)\n    cv_results.append({\"model\":\"LightGBM(es)\", \"rmse_mean\": rmse_mean, \"rmse_std\": rmse_std, \"estimator\": tuned_lgbm, \"name\":\"LightGBM(es)\"})\n\n# Summary table\nres_df = pd.DataFrame([{k:v for k,v in r.items() if k in (\"model\",\"rmse_mean\",\"rmse_std\")} for r in cv_results]) \\\n           .sort_values(\"rmse_mean\")\ndisplay(res_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:19:48.017570Z","iopub.execute_input":"2025-09-06T19:19:48.018072Z","iopub.status.idle":"2025-09-06T19:20:03.802637Z","shell.execute_reply.started":"2025-09-06T19:19:48.018051Z","shell.execute_reply":"2025-09-06T19:20:03.801781Z"}},"outputs":[{"name":"stdout","text":"Dummy(mean)        RMSE (CV=5): 26.5033 ± 0.0550\nRidge              RMSE (CV=5): 26.4997 ± 0.0538\nPoly2+Ridge        RMSE (CV=5): 26.5003 ± 0.0538\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 9\n[LightGBM] [Info] Start training from score 119.043944\nLightGBM (fold 1)  RMSE=26.4751  best_iter=69\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011341 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 9\n[LightGBM] [Info] Start training from score 118.998528\nLightGBM (fold 2)  RMSE=26.4895  best_iter=87\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011009 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 9\n[LightGBM] [Info] Start training from score 119.005866\nLightGBM (fold 3)  RMSE=26.5545  best_iter=89\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 9\n[LightGBM] [Info] Start training from score 118.986521\nLightGBM (fold 4)  RMSE=26.5504  best_iter=87\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013148 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 9\n[LightGBM] [Info] Start training from score 118.994707\nLightGBM (fold 5)  RMSE=26.4093  best_iter=48\nLightGBM (early-stop)    RMSE (CV=5): 26.4958 ± 0.0536 | median_iter=87\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          model  rmse_mean  rmse_std\n3  LightGBM(es)  26.495753  0.053614\n1         Ridge  26.499718  0.053788\n2   Poly2+Ridge  26.500272  0.053751\n0   Dummy(mean)  26.503283  0.054986","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse_mean</th>\n      <th>rmse_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>LightGBM(es)</td>\n      <td>26.495753</td>\n      <td>0.053614</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ridge</td>\n      <td>26.499718</td>\n      <td>0.053788</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poly2+Ridge</td>\n      <td>26.500272</td>\n      <td>0.053751</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Dummy(mean)</td>\n      <td>26.503283</td>\n      <td>0.054986</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 6) PICK BEST MODEL & FIT ON FULL TRAIN\nbest = min(cv_results, key=lambda d: d[\"rmse_mean\"])\nbest_name = best[\"name\"]\nbest_est  = best[\"estimator\"]\n\nprint(f\"Best model by CV: {best_name}\")\nbest_est.fit(X, y)\n\n# Keep reference to fitted pipeline/estimator for later\nfitted = best_est\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:20:03.803971Z","iopub.execute_input":"2025-09-06T19:20:03.804290Z","iopub.status.idle":"2025-09-06T19:20:05.113551Z","shell.execute_reply.started":"2025-09-06T19:20:03.804259Z","shell.execute_reply":"2025-09-06T19:20:05.112946Z"}},"outputs":[{"name":"stdout","text":"Best model by CV: LightGBM(es)\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 524164, number of used features: 9\n[LightGBM] [Info] Start training from score 119.034899\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 7) PREDICT TEST & POSTPROCESS\npred = fitted.predict(X_test).astype(\"float32\")\n\n# Clip to train range (avoids crazy tails if any)\npred = np.clip(pred, bpm_min, bpm_max)\n\nsubmit = pd.DataFrame({\n    ID_COL: test[ID_COL].values,\n    TARGET: pred\n})\ndisplay(submit.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:20:05.114133Z","iopub.execute_input":"2025-09-06T19:20:05.114347Z","iopub.status.idle":"2025-09-06T19:20:05.396660Z","shell.execute_reply.started":"2025-09-06T19:20:05.114328Z","shell.execute_reply":"2025-09-06T19:20:05.396010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"       id  BeatsPerMinute\n0  524164      119.179497\n1  524165      118.844025\n2  524166      119.237900\n3  524167      119.215263\n4  524168      119.275696","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>BeatsPerMinute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>524164</td>\n      <td>119.179497</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>524165</td>\n      <td>118.844025</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>524166</td>\n      <td>119.237900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>524167</td>\n      <td>119.215263</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>524168</td>\n      <td>119.275696</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 8) SAVE SUBMISSION\nOUT_PATH = \"/kaggle/working/submission.csv\"\nsubmit.to_csv(OUT_PATH, index=False)\nprint(f\"Saved: {OUT_PATH}\")\n\n# Also save CV summary + chosen model meta\nmeta = {\n    \"cv_results\": res_df.to_dict(orient=\"list\"),\n    \"best_model\": best_name,\n    \"random_state\": RANDOM_STATE,\n    \"features\": numeric_cols,\n    \"bpm_minmax\": [bpm_min, bpm_max]\n}\nPath(\"/kaggle/working/baseline_meta.json\").write_text(json.dumps(meta, indent=2))\nprint(\"Saved: /kaggle/working/baseline_meta.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T19:20:05.398710Z","iopub.execute_input":"2025-09-06T19:20:05.399024Z","iopub.status.idle":"2025-09-06T19:20:05.680047Z","shell.execute_reply.started":"2025-09-06T19:20:05.399004Z","shell.execute_reply":"2025-09-06T19:20:05.679234Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/submit.csv\nSaved: /kaggle/working/baseline_meta.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}