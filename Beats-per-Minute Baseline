{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1) SETUP\nimport os, math, gc, warnings, json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import Ridge\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(context=\"notebook\", style=\"whitegrid\")\npd.set_option(\"display.float_format\", lambda x: f\"{x:,.6f}\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR = \"/kaggle/input/playground-series-s5e9/\"\nTRAIN = os.path.join(DATA_DIR, \"train.csv\")\nTEST  = os.path.join(DATA_DIR, \"test.csv\")\nSUB_SAMP = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\nTARGET = \"BeatsPerMinute\"\nID_COL = \"id\"\n\n# Optional libs\nHAS_LGB = True\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    HAS_LGB = False\n    print(\"LightGBM not available; skipping that model.\")\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\nrmse_scorer = make_scorer(lambda yt, yp: rmse(yt, yp), greater_is_better=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:36.380341Z","iopub.execute_input":"2025-09-07T18:34:36.380648Z","iopub.status.idle":"2025-09-07T18:34:45.051592Z","shell.execute_reply.started":"2025-09-07T18:34:36.380622Z","shell.execute_reply":"2025-09-07T18:34:45.050530Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 2) LOAD DATA & DEFINE FEATURES\ntrain = pd.read_csv(TRAIN)\ntest  = pd.read_csv(TEST)\n\nprint(\"Shapes:\", train.shape, test.shape)\nassert TARGET in train.columns and ID_COL in train.columns and ID_COL in test.columns\n\n# Numeric features only, excluding id/target\nnumeric_cols = [c for c in train.columns \n                if c not in [TARGET, ID_COL] and pd.api.types.is_numeric_dtype(train[c])]\nprint(\"Using numeric features:\", numeric_cols)\n\nX = train[numeric_cols].astype(\"float32\")\ny = train[TARGET].astype(\"float32\").values\nX_test = test[numeric_cols].astype(\"float32\")\n\nbpm_min, bpm_max = float(train[TARGET].min()), float(train[TARGET].max())\nprint(f\"BPM range (train): [{bpm_min:.3f}, {bpm_max:.3f}]\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:45.053354Z","iopub.execute_input":"2025-09-07T18:34:45.054036Z","iopub.status.idle":"2025-09-07T18:34:47.068394Z","shell.execute_reply.started":"2025-09-07T18:34:45.054008Z","shell.execute_reply":"2025-09-07T18:34:47.067206Z"}},"outputs":[{"name":"stdout","text":"Shapes: (524164, 11) (174722, 10)\nUsing numeric features: ['RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality', 'InstrumentalScore', 'LivePerformanceLikelihood', 'MoodScore', 'TrackDurationMs', 'Energy']\nBPM range (train): [46.718, 206.037]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3) CV SETUP + EVALUATOR\nCV_SPLITS = 5\ncv = KFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\ndef evaluate(name, estimator, X, y, cv=cv, scorer=rmse_scorer):\n    scores = -cross_val_score(estimator, X, y, cv=cv, scoring=scorer, n_jobs=-1)\n    print(f\"{name:18s} RMSE (CV={cv.get_n_splits()}): {scores.mean():.4f} ± {scores.std():.4f}\")\n    return {\"model\": name, \"rmse_mean\": scores.mean(), \"rmse_std\": scores.std(), \"estimator\": estimator}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:47.069215Z","iopub.execute_input":"2025-09-07T18:34:47.069480Z","iopub.status.idle":"2025-09-07T18:34:47.076368Z","shell.execute_reply.started":"2025-09-07T18:34:47.069460Z","shell.execute_reply":"2025-09-07T18:34:47.075260Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 3B) FAST CV SAMPLE (bounds runtime)\nFAST_CV = False\nCV_SAMPLE = 300_000   # adjust smaller if needed (e.g., 100_000)\n\nif FAST_CV and len(X) > CV_SAMPLE:\n    cv_idx = np.random.RandomState(42).choice(len(X), CV_SAMPLE, replace=False)\n    X_cv = X.iloc[cv_idx].copy()\n    y_cv = y[cv_idx].copy()\nelse:\n    X_cv = X\n    y_cv = y\n\nprint(f\"CV will use {len(X_cv):,} rows out of {len(X):,}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:47.078614Z","iopub.execute_input":"2025-09-07T18:34:47.078913Z","iopub.status.idle":"2025-09-07T18:34:47.105295Z","shell.execute_reply.started":"2025-09-07T18:34:47.078890Z","shell.execute_reply":"2025-09-07T18:34:47.104250Z"}},"outputs":[{"name":"stdout","text":"CV will use 524,164 rows out of 524,164.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 4) DEFINE FAST MODELS\nmodels = []\n\n# A) Dummy (mean)\nmodels.append((\"Dummy(median)\", DummyRegressor(strategy=\"median\")))\n\n# B) Ridge on raw features (standardized)\nmodels.append((\n    \"Ridge\",\n    Pipeline([\n        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n        (\"ridge\", Ridge(alpha=1.0, random_state=RANDOM_STATE))\n    ])\n))\n\n# C) Ridge on polynomial degree=2 (captures simple interactions; still fast here)\n#    Keep degree=2 to limit feature explosion (≈ features + squares + pairwise inter.).\nmodels.append((\n    \"Poly2+Ridge\",\n    Pipeline([\n        (\"poly\",   PolynomialFeatures(degree=2, include_bias=False)),\n        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n        (\"ridge\",  Ridge(alpha=1.0, random_state=RANDOM_STATE))\n    ])\n))\n\n# D) LightGBM (tree ensemble) — small, fast config\nif HAS_LGB:\n    lgbm = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=2000,          # modest rounds\n        learning_rate=0.02,\n        num_leaves=31,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    )\n    models.append((\"LightGBM\", lgbm))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:47.106536Z","iopub.execute_input":"2025-09-07T18:34:47.107021Z","iopub.status.idle":"2025-09-07T18:34:47.129492Z","shell.execute_reply.started":"2025-09-07T18:34:47.106986Z","shell.execute_reply":"2025-09-07T18:34:47.128187Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 5) FAST CROSS-VALIDATION (RMSE) with bounded data + LGBM early stopping\ncv_results = []\n\ndef evaluate_fast(name, estimator, Xc, yc, cv=cv):\n    # Generic sklearn models via cross_val_score (on sampled rows)\n    scores = -cross_val_score(estimator, Xc, yc, cv=cv, scoring=rmse_scorer, n_jobs=-1)\n    print(f\"{name:18s} RMSE (CV={cv.get_n_splits()}): {scores.mean():.4f} ± {scores.std():.4f}\")\n    return {\"model\": name, \"rmse_mean\": scores.mean(), \"rmse_std\": scores.std(), \"estimator\": estimator}\n\n# A) Dummy\nfor name, est in models:\n    if name != \"LightGBM\":\n        res = evaluate_fast(name, est, X_cv, y_cv, cv=cv)\n        res[\"name\"] = name\n        cv_results.append(res)\n\n# B) LightGBM with early stopping (custom loop)\nif HAS_LGB and any(n == \"LightGBM\" for n, _ in models):\n    params = dict(\n        objective=\"regression\",\n        learning_rate=0.02,\n        num_leaves=31,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    )\n    NUM_BOOST_ROUND = 2000\n    EARLY_STOP = 500\n\n    rmses, iters = [], []\n    for fold, (tr_idx, va_idx) in enumerate(cv.split(X_cv), 1):\n        X_tr, X_va = X_cv.iloc[tr_idx].values, X_cv.iloc[va_idx].values\n        y_tr, y_va = y_cv[tr_idx], y_cv[va_idx]\n\n        model = lgb.LGBMRegressor(**params, n_estimators=NUM_BOOST_ROUND)\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(EARLY_STOP, verbose=False)],\n        )\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n        fold_rmse = rmse(y_va, pred_va)\n        rmses.append(fold_rmse)\n        iters.append(model.best_iteration_)\n\n        print(f\"LightGBM (fold {fold})  RMSE={fold_rmse:.4f}  best_iter={model.best_iteration_}\")\n\n    rmse_mean, rmse_std = float(np.mean(rmses)), float(np.std(rmses))\n    best_iter_median = int(np.median(iters))\n    print(f\"LightGBM (early-stop)    RMSE (CV={cv.get_n_splits()}): {rmse_mean:.4f} ± {rmse_std:.4f} | median_iter={best_iter_median}\")\n\n    # store an unfitted template with tuned n_estimators for final training\n    tuned_lgbm = lgb.LGBMRegressor(**params, n_estimators=best_iter_median)\n    cv_results.append({\"model\":\"LightGBM(es)\", \"rmse_mean\": rmse_mean, \"rmse_std\": rmse_std, \"estimator\": tuned_lgbm, \"name\":\"LightGBM(es)\"})\n\n# Summary table\nres_df = pd.DataFrame([{k:v for k,v in r.items() if k in (\"model\",\"rmse_mean\",\"rmse_std\")} for r in cv_results]) \\\n           .sort_values(\"rmse_mean\")\ndisplay(res_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:34:47.130650Z","iopub.execute_input":"2025-09-07T18:34:47.131000Z","iopub.status.idle":"2025-09-07T18:35:34.270076Z","shell.execute_reply.started":"2025-09-07T18:34:47.130973Z","shell.execute_reply":"2025-09-07T18:35:34.268981Z"}},"outputs":[{"name":"stdout","text":"Dummy(median)      RMSE (CV=5): 26.4696 ± 0.0402\nRidge              RMSE (CV=5): 26.4662 ± 0.0397\nPoly2+Ridge        RMSE (CV=5): 26.4662 ± 0.0393\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025636 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 9\n[LightGBM] [Info] Start training from score 119.056554\nLightGBM (fold 1)  RMSE=26.4372  best_iter=146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033463 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 9\n[LightGBM] [Info] Start training from score 119.039042\nLightGBM (fold 2)  RMSE=26.4832  best_iter=120\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 9\n[LightGBM] [Info] Start training from score 119.033031\nLightGBM (fold 3)  RMSE=26.5243  best_iter=83\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022894 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 9\n[LightGBM] [Info] Start training from score 119.023957\nLightGBM (fold 4)  RMSE=26.4430  best_iter=143\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 419332, number of used features: 9\n[LightGBM] [Info] Start training from score 119.021913\nLightGBM (fold 5)  RMSE=26.4077  best_iter=108\nLightGBM (early-stop)    RMSE (CV=5): 26.4591 ± 0.0405 | median_iter=120\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"           model  rmse_mean  rmse_std\n3   LightGBM(es)  26.459081  0.040502\n2    Poly2+Ridge  26.466202  0.039301\n1          Ridge  26.466208  0.039725\n0  Dummy(median)  26.469614  0.040153","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse_mean</th>\n      <th>rmse_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>LightGBM(es)</td>\n      <td>26.459081</td>\n      <td>0.040502</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poly2+Ridge</td>\n      <td>26.466202</td>\n      <td>0.039301</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ridge</td>\n      <td>26.466208</td>\n      <td>0.039725</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Dummy(median)</td>\n      <td>26.469614</td>\n      <td>0.040153</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 6) PICK BEST MODEL & FIT ON FULL TRAIN\nbest = min(cv_results, key=lambda d: d[\"rmse_mean\"])\nbest_name = best[\"name\"]\nbest_est  = best[\"estimator\"]\n\nprint(f\"Best model by CV: {best_name}\")\nbest_est.fit(X, y)\n\n# Keep reference to fitted pipeline/estimator for later\nfitted = best_est\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:35:34.271016Z","iopub.execute_input":"2025-09-07T18:35:34.271279Z","iopub.status.idle":"2025-09-07T18:35:36.471459Z","shell.execute_reply.started":"2025-09-07T18:35:34.271258Z","shell.execute_reply":"2025-09-07T18:35:36.470521Z"}},"outputs":[{"name":"stdout","text":"Best model by CV: LightGBM(es)\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2295\n[LightGBM] [Info] Number of data points in the train set: 524164, number of used features: 9\n[LightGBM] [Info] Start training from score 119.034899\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 7) PREDICT TEST & POSTPROCESS\npred = fitted.predict(X_test).astype(\"float32\")\n\n# Clip to train range (avoids crazy tails if any)\npred = np.clip(pred, bpm_min, bpm_max)\n\nsubmit = pd.DataFrame({\n    ID_COL: test[ID_COL].values,\n    TARGET: pred\n})\ndisplay(submit.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:35:36.472310Z","iopub.execute_input":"2025-09-07T18:35:36.473340Z","iopub.status.idle":"2025-09-07T18:35:36.926443Z","shell.execute_reply.started":"2025-09-07T18:35:36.473307Z","shell.execute_reply":"2025-09-07T18:35:36.925599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"       id  BeatsPerMinute\n0  524164      119.211678\n1  524165      118.799858\n2  524166      119.356506\n3  524167      119.252602\n4  524168      119.306847","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>BeatsPerMinute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>524164</td>\n      <td>119.211678</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>524165</td>\n      <td>118.799858</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>524166</td>\n      <td>119.356506</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>524167</td>\n      <td>119.252602</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>524168</td>\n      <td>119.306847</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 8) SAVE SUBMISSION\nOUT_PATH = \"/kaggle/working/submission.csv\"\nsubmit.to_csv(OUT_PATH, index=False)\nprint(f\"Saved: {OUT_PATH}\")\n\n# Also save CV summary + chosen model meta\nmeta = {\n    \"cv_results\": res_df.to_dict(orient=\"list\"),\n    \"best_model\": best_name,\n    \"random_state\": RANDOM_STATE,\n    \"features\": numeric_cols,\n    \"bpm_minmax\": [bpm_min, bpm_max]\n}\nPath(\"/kaggle/working/baseline_meta.json\").write_text(json.dumps(meta, indent=2))\nprint(\"Saved: /kaggle/working/baseline_meta.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:35:36.927161Z","iopub.execute_input":"2025-09-07T18:35:36.927383Z","iopub.status.idle":"2025-09-07T18:35:37.214557Z","shell.execute_reply.started":"2025-09-07T18:35:36.927365Z","shell.execute_reply":"2025-09-07T18:35:37.213513Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/submission.csv\nSaved: /kaggle/working/baseline_meta.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}